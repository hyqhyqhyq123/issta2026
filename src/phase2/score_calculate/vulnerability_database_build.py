import json
import logging
import re
from pathlib import Path
from typing import List, Dict, Any


logger = logging.getLogger(__name__)


class VulnerabilityDatabase:
    """Minimal vulnerability database builder"""

    def __init__(self) -> None:
        self.vulnerability_patterns: List[Dict[str, Any]] = []

    @staticmethod
    def _strip_comments(code: str) -> str:
        s = code
        n = len(s)
        i = 0
        out = []
        in_line = False
        in_block = False
        in_str = False
        in_char = False
        esc = False

        while i < n:
            ch = s[i]
            nx = s[i + 1] if i + 1 < n else ''

            if in_line:
                if ch == '\n':
                    in_line = False
                    out.append('\n')
                i += 1
                continue

            if in_block:
                if ch == '*' and nx == '/':
                    in_block = False
                    i += 2
                else:
                    if ch == '\n':
                        out.append('\n')
                    i += 1
                continue

            if in_str:
                out.append(ch)
                if esc:
                    esc = False
                elif ch == '\\':
                    esc = True
                elif ch == '"':
                    in_str = False
                i += 1
                continue

            if in_char:
                out.append(ch)
                if esc:
                    esc = False
                elif ch == '\\':
                    esc = True
                elif ch == "'":
                    in_char = False
                i += 1
                continue

            if ch == '/' and nx == '/':
                in_line = True
                i += 2
                continue
            if ch == '/' and nx == '*':
                in_block = True
                i += 2
                continue
            if ch == '"':
                in_str = True
                out.append(ch)
                i += 1
                continue
            if ch == "'":
                in_char = True
                out.append(ch)
                i += 1
                continue
            out.append(ch)
            i += 1
        return ''.join(out)

    def build_from_vul_db(self, input_path: str = "data/vul_db.jsonl") -> None:
        """Read func from specified JSONL, remove comments and save to memory.

        Only keep core information: {"code": <code after removing comments>}.
        """
        p = Path(input_path)
        if not p.exists():
            raise FileNotFoundError(f"Input file not found: {p}")

        kept: List[Dict[str, Any]] = []
        with p.open("r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                except json.JSONDecodeError:
                    logger.debug("Line %d JSON parsing failed, skipped", line_num)
                    continue

                func = obj.get("func") or obj.get("code")
                if not isinstance(func, str) or not func.strip():
                    continue
                cleaned = self._strip_comments(func)
                kept.append({
                    "idx": obj.get("idx"),
                    "target": obj.get("target"),
                    "func_name": obj.get("func_name"),
                    "func": cleaned,
                })

        self.vulnerability_patterns = kept
        logger.info("Built pattern count: %d", len(self.vulnerability_patterns))

    def save_as_jsonl(self, output_path: str = "data/vul_db_clean.jsonl") -> None:
        """Save cleaned results in memory as JSONL."""
        out = Path(output_path)
        out.parent.mkdir(parents=True, exist_ok=True)
        with out.open("w", encoding="utf-8") as w:
            for item in self.vulnerability_patterns:
                w.write(json.dumps(item, ensure_ascii=False) + "\n")
        logger.info("Saved to: %s", out)

    def vectorize_to_npy(
        self,
        output_npy: str = "",
        model_name: str = "",
        batch_size: int = 16,
        max_length: int = 512,
        input_path: str | None = "",
        strip_comments: bool = False,
    ) -> None:
        """Vectorize code using CodeBERT and save as .npy.

        - Prefer self.vulnerability_patterns in memory; if empty and input_path provided, read from JSONL
        - Mean pooling of last_hidden_state as sentence vector
        """
        texts: List[str] = []
        if self.vulnerability_patterns:
            texts = [rec.get("func", "") for rec in self.vulnerability_patterns]
        elif input_path and Path(input_path).exists():
            with Path(input_path).open("r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        obj = json.loads(line)
                    except json.JSONDecodeError:
                        continue
                    code = obj.get("func") or ""
                    if isinstance(code, str) and code.strip():
                        norm = self._strip_comments(code) if strip_comments else code
                        texts.append(norm)
        else:
            raise ValueError("No vectorizable data: please build first or provide valid input_path")

        import numpy as np
        import torch
        from transformers import AutoTokenizer, AutoModel
        from tqdm import tqdm

        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        model.to(device)
        model.eval()

        vectors: List["np.ndarray"] = []
        with torch.no_grad():
            pbar = tqdm(total=len(texts), desc="Embedding", unit="sample")
            for i in range(0, len(texts), batch_size):
                batch = texts[i : i + batch_size]
                inputs = tokenizer(
                    batch,
                    return_tensors="pt",
                    truncation=True,
                    padding=True,
                    max_length=max_length,
                )
                inputs = {k: v.to(device) for k, v in inputs.items()}
                outputs = model(**inputs)
                emb = outputs.last_hidden_state.mean(dim=1).cpu().numpy()
                vectors.append(emb)
                pbar.update(len(batch))
            pbar.close()

        arr = np.vstack(vectors)
        arr_mean = arr.mean(axis=0, keepdims=True)
        arr_centered = arr - arr_mean

        out_p = Path(output_npy)
        out_p.parent.mkdir(parents=True, exist_ok=True)
        np.save(out_p, arr_centered)
        mean_path = out_p.with_name(out_p.stem + "_mean.npy")
        np.save(mean_path, arr_mean)
        logger.info("Written centered embeddings: %s, shape: %s; mean: %s", out_p, arr_centered.shape, mean_path)


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(levelname)s | %(message)s")
    db = VulnerabilityDatabase()
    db.build_from_vul_db("")
    db.save_as_jsonl("")
    db.vectorize_to_npy("")
    


